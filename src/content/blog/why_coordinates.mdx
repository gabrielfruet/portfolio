---
title: "Why Coordinates Aren't Enough"
description: "Why we need IoU based loss functions."
pubDate: "2026-02-02"
tags: ["deep-learning", "object-detection", "loss-function"]
---

import Figure from '../../components/Figure.astro';

# Why do we need IoU based loss functions?

A question that may arise when entering the world of object detection world is:
why does IoU based loss functions perfom better than L1 loss?

In our mind we will generally think of object detection as purely a regression problem. So if we minimize the difference,
L1 in our case, or L2 as another example, between the predicted and the ground truth bounding box, we will get a good model.

And that's sort of true, but not entirely. Since we evaluate the model using
metrics such as mAP, we want to minimize the difference between the predicted
and the ground truth bounding box in terms of IoU. In general, the L1 loss
**does** that, but not directly.

# Loss landscape

When training a model, we want to minimize the loss function, so our model is a "point" in the loss landscape, and we want to move that point to the minimum. 
This movement is entirely dependent on how the gradient behaves in the neighborhood of the current point.

In simple terms, the shape of the loss curve is what determines how the model learns.

## The ball example

If we think of our model as a ball in the loss landscape, we want to move it to minimum. And let's image we have gravity there.

If we release the ball, it will roll down to the a minimum, local or global. That's how optimizers works (e.g. SGD, Adam, etc.) in very simple terms.

<Figure src="../src/assets/images/experiments/object_detection/F1.large.jpg" caption="Figure 1: Loss Landscape" className="w-1/2 mx-auto" source="https://science.sciencemag.org/content/360/6388/478/tab-figures-data" />

# L1 Loss

The simplest loss used for object detection is L1 loss for example. Where we are
only minimizing the difference between the predicted and the ground truth
bounding box in XY terms, not WH (This is because we want to visualize in a 2D plane.)

<Figure src="../src/assets/images/experiments/object_detection/l1_loss_landscape.png" caption="Figure 2: L1 Loss Landscape" className="w-1/2 mx-auto" />

As we can see, the loss landscape of L1 loss is very smooth, and the gradient is
always pointing to the minimum. But the loss is very "flat", so the ball will
roll down very consistently to find the minimum.

# IoU Loss

The IoU loss is defined as $1 - IoU$. It is not very used, but it is a good
starting point to understand the problem.

<Figure src="../src/assets/images/experiments/object_detection/iou_loss_landscape.png" caption="Figure 3: IoU Loss Landscape" className="w-1/2 mx-auto" />

This loss is not used in practice because it is not smooth, and the gradient is
not always pointing to the minimum.

Take a look at the borders, we have a flat surface, which provides no gradient.
Only on the center, when the gt box and pred box has some overlap, is when we
start having gradients. This is a problem because if the model is far from the
ground truth, it will not have any gradient to guide it to the minimum.

> The IoU of non-overlapping boxes is 0.

So we need something that says 

> Hey, i know that you are not overlapping the ground truth, but go in this direction, and you will get there.

And that's what GIoU, DIoU, and CIoU loss functions do.

# Complete IoU Loss

Initially proposed by the paper [Enhancing Geometric Factors in Model Learning and Inference for Object Detection and Instance Segmentation](https://arxiv.org/pdf/2005.03572), 

It is defined as 

$$
L   _{CIoU} = S(b, b^{gt}) + D(b, b^{gt}) + V(b, b^{gt})
$$
$$
S(b, b^{gt}) = 1 - IoU(b, b^{gt})
$$
$$
D(b, b^{gt}) = \frac{\rho^2(b, b^{gt})}{c^2}
$$
$$
V(b, b^{gt}) = \frac{4}{\pi^2} (\arctan \frac{w^{gt}}{h^{gt}} - \arctan \frac{w}{h})^2
$$
$$
\alpha = 
\begin{cases}
  \frac{V}{1 - IoU + V} & \text{if $IoU \geq 0.5$} \\
  0 & \text{if $IoU < 0.5$}
\end{cases}
$$

Where:

- $IoU$ is the Intersection over Union.
- $\rho^2(b, b^{gt})$ is the squared Euclidean distance between the center points of the predicted and ground truth boxes.
- $c$ is the diagonal length of the smallest enclosing box that contains both the predicted and ground truth boxes.
- $\alpha$ is a weighting factor.
- $V$ is a parameter that measures the consistency of the aspect ratios.

The CIoU loss landscape is very smooth, and the gradient is always pointing to the minimum.

<Figure src="../src/assets/images/experiments/object_detection/ciou_loss_landscape.png" caption="Figure 4: CIoU Loss Landscape" className="w-1/2 mx-auto" />

# Optimization Process

Here is a visualization of how the bounding box regression works during the optimization process.

<Figure src="../src/assets/images/experiments/object_detection/l1_vs_ciou_aspect_ratio.gif" caption="Figure 5: Bounding Box Optimization" className="w-1/2 mx-auto" />